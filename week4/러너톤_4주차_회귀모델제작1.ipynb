{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMgrPtGlU2h/jN3J0poZ9Mo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 원본 문제"],"metadata":{"id":"PL66OpG0jP0s"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qeQNYIb9CIW3"},"outputs":[],"source":["\"\"\"## DNN(MLP) 모델을 이용한 보스터 집값 회귀 분석\n","* 체점 기준 :\n","  - 데이터 셋 : 체점 서버 내 테스트 데이터 셋\n","  - 성능 지표 : MAE\n","  - PASS 기준 : 3.0 이하\n","  \"\"\"\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from tensorflow.keras.datasets import boston_housing\n","\n","\"\"\"* Step 1. Inptu tensor 와 Target tensor 준비(훈련데이터)\"\"\"\n","\n","# 수강생 작성 코드\n","# 1. import 한 boston_housing API를 이용하여 boston_housing 데이터 셋을 다운로드 받으세요)\n","\n","\n","\"\"\"* Step 2. 데이터의 전처리 \"\"\"\n","\n","# 수강생 작성 코드\n","# 1. train 데이터의 feature 별 평균값, 표준편차 값을 이용하여 정규화 작업을 진행하세요\n","\n","\n","\n","\"\"\"* Step 3. batch 구성을 위한 데이터 pipeline 생성\"\"\"\n","\n","# 수강생 작성 코드\n","# 1. 훈련 데이터, 테스트 데이터를 파이토치의 텐서 형태로 변환해주세요\n","\n","\n","# 수강생 작성 코드\n","# 1. torch.utils.data.TensorDataset API를 활용해 훈련, 검증, 테스트 데이터를 위한 Dataset 객체를 생성 하세요\n","# 2. 훈련 Dataset을 torch.utils.data.random_split() API 를 이용해 5:1로 분할하여 검증 Dataset객체를 생성 하세요\n","\n","\n","# 수강생 작성 코드\n","# 1. torch.utils.data.DataLoader API를 활용해 훈련, 검증, 테스트 데이터 pipeline을 생성하세요\n","\n","\n","\n","\"\"\"* Step 4. DNN(MLP) 모델 디자인\"\"\"\n","\n","# 수강생 작성 코드\n","# 1. torch.nn.Module API를 이용하여 보스턴 집값 회귀 분석 데이터 셋을 분석 하기 위한 MLP 모델 class를 정의 하세요\n","# 2. 정의한 MLP 모델 class를 이용하여 모델 객체를 생성 하세요\n","\n","\n","\n","\"\"\"* Step 5. 모델의 학습 정보 설정\"\"\"\n","\n","# 수강생 작성 코드\n","# 1. torch.nn 모듈에 구현된 API를 이용해 회귀를 위한 손실함수 객체를 생성하세요\n","# 2. torch.nn 모듈에 구현된 API를 이용해 체점 기준 MAE 를 계산하기 위한 객체를 생성하세요\n","# 3. torch.optim API로 옵티마이저를 설정하세요\n","\n","\n","\"\"\"* Step 6. 모델의 학습 loop 를 정의하는 함수 생성\"\"\"\n","\n","def train(epoch):\n","  # 수강생 작성 코드\n","  # 1. 모델 객체를 학습에 적합한 상태로 변경하세요\n","\n","\n","  # 수강생 작성 코드\n","  # 1. 학습데이터 에서 미니 배치를 추출하여 학습 loop를 수행하는 for문을 작성 하세요\n","  for batch_idx,(____, ____) in enumerate(____):\n","    # 수강생 작성 코드\n","    # 1. 모델에 input data를 전달하여 순전파를 수행하세요\n","\n","\n","    # 수강생 작성 코드\n","    # 1. loss 함수에 모델의 예측값 과 정답 정보를 전달하여 loss 값을 계산 하세요\n","\n","\n","    # 수강생 작성 코드\n","    # 1. mae 계산 함수에 모델의 예측값 과 정답 정보를 전달하여 회귀 분석의 성능지표 MAE 계산 하세요\n","\n","\n","    # 수강생 작성 코드\n","    # 1. optimizer 객체를 이용해 모델을 구성하는 파라미터들(w, b)의 gradient를 초기화 하세요\n","\n","\n","    # 수강생 작성 코드\n","    # 1. 모델을 구성하는 파라미터들(w, b)이 loss 에 미치는 영향도(gradient)를 계산 하세요\n","\n","\n","    # 수강생 작성 코드\n","    # 1. 계산된 영향도(gradient) 값을 이용하여 모델의 파라미터(w,b)를 업데이트 하세요\n","\n","\n","    # 모델의 성능 지표를 계산 및 출력 하여 학습 진행을 체크하는 코드\n","    if batch_idx%50 == 0:\n","      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tMAE: {:.6f}\\tMSE: {:.06f}'.format(\n","            epoch,\n","            (batch_idx+1) * len(data),\n","            len(train_loader.dataset),\n","            100. * (batch_idx+1) / len(train_loader),\n","            batch_mae,\n","            batch_mse\n","            ))\n","\n","\n","\"\"\"* Step 7. 모델의 검증 loop 를 정의하는 함수 생성\"\"\"\n","\n","def test(data_loader):\n","  # 수강생 작성 코드\n","  # 1. 모델 객체를 테스트에 적합한 상태로 변경하세요\n","\n","\n","  # 수강생 작성 코드\n","  # 1. 모델의 성능을 지표를 계산에 필요한 정보 수집을 위한 변수 선언\n","  #   - loss 를 수집하기 위한 'test_mse' 변수 선언 및 0 으로 초기화\n","  #   - MAE 를 수집하기 위한 'test_mae' 변수 선언 및 0 으로 초기화\n","\n","\n","  # 수강생 작성 코드\n","  # 1. 검증데이터 에서 미니 배치를 추출하여 검증 loop를 수행하는 for문을 작성 하세요\n","  for ____, ____ in ____:\n","    # 수강생 작성 코드\n","    # 1. 모델에 input data를 전달하여 순전파를 수행하세요\n","\n","\n","    # 수강생 작성 코드\n","    # 1. loss 함수에 모델의 예측값 과 정답 정보를 전달하여 모델의 성능을 검증하기 위한 loss 값을 계산 하세요\n","    #   - 배치별로 계산된 loss 정보를 'test_mse' 변수에 누적\n","    # 2. MAE 함수에 모델의 예측값 과 정답 정보를 전달하여 모델의 성능을 검증하기 위한 MAE 값을 계산 하세요\n","    #   - 배치별로 계산된 MAE 정보를 'test_mae' 변수에 누적\n","\n","\n","  # 수강생 작성 코드\n","  # 1. 'test_mae' 에 누적된 배치 별 MAE의 합을 배치의 개수로 나누에 전체 데이터 셋의 평균 MAE를 계산 하세요\n","  #    - 계산 된 평균 MAE를 'test_mae' 변수에 할당\n","  # 2. 'test_mse' 에 누적된 배치 별 MAE의 합을 배치의 개수로 나누에 전체 데이터 셋의 평균 loss를 계산 하세요\n","  #    - 계산 된 평균 MAE를 'test_mse' 변수에 할당\n","\n","  # 모델의 성능 지표를 계산 및 출력 하여 학습 진행을 체크하는 코드\n","  print('\\nTest set: Average MAE: {:.4f}\\t MSE: {:.4f}\\n'.format(\n","        test_mae,\n","        test_mse\n","        ))\n","\n","\n","\"\"\"* Step 5. 훈련 함수와 검증 함수로 모델 훈련 및 검증\"\"\"\n","# 수강생 작성 코드\n","# 1. 반복문을 활용해 epoch 수 만큼 훈련 및 검증을 하세요.\n","\n","\n","# 수강생 작성 코드\n","# 1. 테스트 데이터 셋을 이용해 모델의 성능을 확인 하세요\n"]},{"cell_type":"markdown","source":["# 풀이"],"metadata":{"id":"lbnkdTw_jUDP"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from tensorflow.keras.datasets import boston_housing"],"metadata":{"id":"eataoEc5YqTb","executionInfo":{"status":"ok","timestamp":1741610917734,"user_tz":-540,"elapsed":396,"user":{"displayName":"김민정","userId":"00228132003151151484"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["\"\"\"* Step 1. Inptu tensor 와 Target tensor 준비(훈련데이터)\"\"\"\n","\n","# 수강생 작성 코드\n","# 1. import 한 boston_housing API를 이용하여 boston_housing 데이터 셋을 다운로드 받으세요\n","(train_x, train_y), (test_x, test_y) = boston_housing.load_data()"],"metadata":{"id":"wBuYSBh3YstG","executionInfo":{"status":"ok","timestamp":1741610918024,"user_tz":-540,"elapsed":12,"user":{"displayName":"김민정","userId":"00228132003151151484"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["\"\"\"* Step 2. 데이터의 전처리 \"\"\"\n","\n","# 수강생 작성 코드\n","# 1. train 데이터의 feature 별 평균값, 표준편차 값을 이용하여 정규화 작업을 진행하세요\n","# test, train 독립변수는 normalization을 해야함 but 종속변수는 할 필요x\n","# 평균값 및 표준편차 값 구하기\n","x_mean, x_std = train_x.mean(axis=0), train_x.std(axis=0)\n","\n","# 정규화 작업\n","train_x = (train_x - x_mean)/x_std\n","\n","test_x = (test_x - x_mean)/x_std"],"metadata":{"id":"Djuub0EcY3OP","executionInfo":{"status":"ok","timestamp":1741610918110,"user_tz":-540,"elapsed":10,"user":{"displayName":"김민정","userId":"00228132003151151484"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["\"\"\"* Step 3. batch 구성을 위한 데이터 pipeline 생성\"\"\"\n","\n","# 수강생 작성 코드\n","# 1. 훈련 데이터, 테스트 데이터를 파이토치의 텐서 형태로 변환해주세요\n","# 회귀 문제에서는 값을 예측하는 것 -> float 대신에 long\n","# nn.Linear가 2D tensor을 요구하기 때문에 (batch_size, output_dim) 차원을 1D -> 2D\n","train_x, train_y = torch.Tensor(train_x), torch.Tensor(train_y).view(-1,1)\n","test_x, test_y = torch.Tensor(test_x), torch.Tensor(test_y).view(-1,1)\n","\n","# 수강생 작성 코드\n","# 1. torch.utils.data.TensorDataset API를 활용해 훈련, 검증, 테스트 데이터를 위한 Dataset 객체를 생성 하세요\n","# 2. 훈련 Dataset을 torch.utils.data.random_split() API 를 이용해 5:1로 분할하여 검증 Dataset객체를 생성 하세요\n","train_dataset = torch.utils.data.TensorDataset(train_x, train_y )\n","train_dataset_count = int(len(train_dataset)*(5/6))\n","val_dataset_count = len(train_dataset) - train_dataset_count\n","\n","train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_dataset_count, val_dataset_count])\n","test_dataset = torch.utils.data.TensorDataset(test_x, test_y)\n","\n","# 수강생 작성 코드\n","# 1. torch.utils.data.DataLoader API를 활용해 훈련, 검증, 테스트 데이터 pipeline을 생성하세요\n","batch_size = 8\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True)\n","val_loader = torch.utils.data.DataLoader(val_dataset,\n","                                         batch_size=batch_size,\n","                                         shuffle=False)\n","test_loader = torch.utils.data.DataLoader(test_dataset,\n","                                         batch_size=batch_size,\n","                                         shuffle=False)"],"metadata":{"id":"zYZoxe44ZK29","executionInfo":{"status":"ok","timestamp":1741611609596,"user_tz":-540,"elapsed":8,"user":{"displayName":"김민정","userId":"00228132003151151484"}}},"execution_count":88,"outputs":[]},{"cell_type":"code","source":["train_x.shape, test_x.shape, train_y.shape, test_y.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0epDsv7IkNY8","executionInfo":{"status":"ok","timestamp":1741611610310,"user_tz":-540,"elapsed":38,"user":{"displayName":"김민정","userId":"00228132003151151484"}},"outputId":"1f32aabf-aa56-4193-c2d7-6a03b0fcc9ff"},"execution_count":89,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([404, 13]),\n"," torch.Size([102, 13]),\n"," torch.Size([404, 1]),\n"," torch.Size([102, 1]))"]},"metadata":{},"execution_count":89}]},{"cell_type":"code","source":["x, y =next(iter(train_loader))"],"metadata":{"id":"Amx_eCg5lEtN","executionInfo":{"status":"ok","timestamp":1741611610618,"user_tz":-540,"elapsed":35,"user":{"displayName":"김민정","userId":"00228132003151151484"}}},"execution_count":90,"outputs":[]},{"cell_type":"code","source":["x.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lTdVDpKSlgii","executionInfo":{"status":"ok","timestamp":1741611611235,"user_tz":-540,"elapsed":575,"user":{"displayName":"김민정","userId":"00228132003151151484"}},"outputId":"2d0b9580-a759-434d-e3b8-074431f81b54"},"execution_count":91,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([8, 13])"]},"metadata":{},"execution_count":91}]},{"cell_type":"code","source":["\"\"\"* Step 4. DNN(MLP) 모델 디자인\"\"\"\n","\n","# 수강생 작성 코드\n","# 1. torch.nn.Module API를 이용하여 보스턴 집값 회귀 분석 데이터 셋을 분석 하기 위한 MLP 모델 class를 정의 하세요\n","# 2. 정의한 MLP 모델 class를 이용하여 모델 객체를 생성 하세요\n","class Model_boston(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.fc1 = nn.Linear(13, 64)\n","    self.fc2 = nn.Linear(64, 32)\n","    self.fc3 = nn.Linear(32, 1)\n","\n","  def forward(self, x):\n","      x = F.relu(self.fc1(x))\n","      x = F.relu(self.fc2(x))\n","      x = self.fc3(x)\n","      return x"],"metadata":{"id":"Qsy2REaac9ol","executionInfo":{"status":"ok","timestamp":1741611611385,"user_tz":-540,"elapsed":7,"user":{"displayName":"김민정","userId":"00228132003151151484"}}},"execution_count":92,"outputs":[]},{"cell_type":"code","source":["# 수강생 작성 코드\n","# 1. torch.nn 모듈에 구현된 API를 이용해 회귀를 위한 손실함수 객체를 생성하세요\n","# 2. torch.nn 모듈에 구현된 API를 이용해 체점 기준 MAE 를 계산하기 위한 객체를 생성하세요\n","# 3. torch.optim API로 옵티마이저를 설정하세요\n","\n","model = Model_boston()\n","criterion_mse = nn.MSELoss()\n","criterion_mae = nn.L1Loss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"],"metadata":{"id":"q2tHFaKHrUCr","executionInfo":{"status":"ok","timestamp":1741611611433,"user_tz":-540,"elapsed":10,"user":{"displayName":"김민정","userId":"00228132003151151484"}}},"execution_count":93,"outputs":[]},{"cell_type":"code","source":["\"\"\"* Step 6. 모델의 학습 loop 를 정의하는 함수 생성\"\"\"\n","def train(epoch):\n","  # 수강생 작성 코드\n","  # 1. 모델 객체를 학습에 적합한 상태로 변경하세요\n","  model.train()\n","\n","  batch_mse, batch_mae = 0, 0\n","  # 수강생 작성 코드\n","  # 1. 학습데이터 에서 미니 배치를 추출하여 학습 loop를 수행하는 for문을 작성 하세요\n","  for batch_idx,(data, target) in enumerate(train_loader):\n","    # 수강생 작성 코드\n","    # 1. 모델에 input data를 전달하여 순전파를 수행하세요\n","    # model.forward를 할 필요x : nn.module이 __call__() method를 통해 foward를 자동으로 불러줌\n","    output = model(data)\n","\n","    # 수강생 작성 코드\n","    # 1. loss 함수에 모델의 예측값 과 정답 정보를 전달하여 loss 값을 계산 하세요\n","    batch_mse = criterion_mse(output, target)\n","\n","    # 수강생 작성 코드\n","    # 1. mae 계산 함수에 모델의 예측값 과 정답 정보를 전달하여 회귀 분석의 성능지표 MAE 계산 하세요\n","    batch_mae = criterion_mae(output, target)\n","\n","    # 수강생 작성 코드\n","    # 1. optimizer 객체를 이용해 모델을 구성하는 파라미터들(w, b)의 gradient를 초기화 하세요\n","    optimizer.zero_grad()\n","\n","    # 수강생 작성 코드\n","    # 1. 모델을 구성하는 파라미터들(w, b)이 loss 에 미치는 영향도(gradient)를 계산 하세요\n","    batch_mse.backward()\n","\n","    # 수강생 작성 코드\n","    # 1. 계산된 영향도(gradient) 값을 이용하여 모델의 파라미터(w,b)를 업데이트 하세요\n","    optimizer.step()\n","\n","    # 모델의 성능 지표를 계산 및 출력 하여 학습 진행을 체크하는 코드\n","    if batch_idx%50 == 0:\n","      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tMAE: {:.6f}\\tMSE: {:.06f}'.format(\n","            epoch,\n","            (batch_idx+1) * len(data),\n","            len(train_loader.dataset),\n","            100. * (batch_idx+1) / len(train_loader),\n","            batch_mae,\n","            batch_mse\n","            ))\n"],"metadata":{"id":"zOUsM4u8p9GT","executionInfo":{"status":"ok","timestamp":1741611611991,"user_tz":-540,"elapsed":22,"user":{"displayName":"김민정","userId":"00228132003151151484"}}},"execution_count":94,"outputs":[]},{"cell_type":"code","source":["def test(data_loader):\n","  # 수강생 작성 코드\n","  # 1. 모델 객체를 테스트에 적합한 상태로 변경하세요\n","  model.eval()\n","\n","  # 수강생 작성 코드\n","  # 1. 모델의 성능을 지표를 계산에 필요한 정보 수집을 위한 변수 선언\n","  #   - loss 를 수집하기 위한 'test_mse' 변수 선언 및 0 으로 초기화\n","  #   - MAE 를 수집하기 위한 'test_mae' 변수 선언 및 0 으로 초기화\n","  test_mse =0\n","  test_mae = 0\n","\n","  # 수강생 작성 코드\n","  # 1. 검증데이터 에서 미니 배치를 추출하여 검증 loop를 수행하는 for문을 작성 하세요\n","  for data, target in data_loader:\n","    # 수강생 작성 코드\n","    # 1. 모델에 input data를 전달하여 순전파를 수행하세요\n","    output = model(data)\n","\n","    # 수강생 작성 코드\n","    # 1. loss 함수에 모델의 예측값 과 정답 정보를 전달하여 모델의 성능을 검증하기 위한 loss 값을 계산 하세요\n","    #   - 배치별로 계산된 loss 정보를 'test_mse' 변수에 누적\n","    # 2. MAE 함수에 모델의 예측값 과 정답 정보를 전달하여 모델의 성능을 검증하기 위한 MAE 값을 계산 하세요\n","    #   - 배치별로 계산된 MAE 정보를 'test_mae' 변수에 누적\n","    test_mse += criterion_mse(output, target).item()\n","    test_mae += criterion_mae(output, target).item()\n","\n","  # 수강생 작성 코드\n","  # 1. 'test_mae' 에 누적된 배치 별 MAE의 합을 배치의 개수로 나누에 전체 데이터 셋의 평균 MAE를 계산 하세요\n","  #    - 계산 된 평균 MAE를 'test_mae' 변수에 할당\n","  # 2. 'test_mse' 에 누적된 배치 별 MAE의 합을 배치의 개수로 나누에 전체 데이터 셋의 평균 loss를 계산 하세요\n","  #    - 계산 된 평균 MAE를 'test_mse' 변수에 할당\n","  test_mae /= len(data_loader)\n","  test_mse /= len(data_loader)\n","\n","  # 모델의 성능 지표를 계산 및 출력 하여 학습 진행을 체크하는 코드\n","  print('\\nTest set: Average MAE: {:.4f}\\t MSE: {:.4f}\\n'.format(\n","        test_mae,\n","        test_mse\n","        ))"],"metadata":{"id":"_y5CHh22xUSw","executionInfo":{"status":"ok","timestamp":1741611612738,"user_tz":-540,"elapsed":23,"user":{"displayName":"김민정","userId":"00228132003151151484"}}},"execution_count":95,"outputs":[]},{"cell_type":"code","source":["\"\"\"* Step 7. 훈련 함수와 검증 함수로 모델 훈련 및 검증\"\"\"\n","# 수강생 작성 코드\n","# 1. 반복문을 활용해 epoch 수 만큼 훈련 및 검증을 하세요.\n","epochs = 150\n","for epoch in range(1,epochs+1):\n","  train(epoch)\n","  test(val_loader)\n","\n","# 수강생 작성 코드\n","# 1. 테스트 데이터 셋을 이용해 모델의 성능을 확인 하세요\n","test(test_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XynXBvvQxkaD","executionInfo":{"status":"ok","timestamp":1741611624066,"user_tz":-540,"elapsed":10865,"user":{"displayName":"김민정","userId":"00228132003151151484"}},"outputId":"bed30dcb-7032-4b53-c27f-590a89e6cfc7"},"execution_count":96,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 1 [8/336 (2%)]\tMAE: 17.929916\tMSE: 344.656250\n","\n","Test set: Average MAE: 22.5903\t MSE: 609.8355\n","\n","Train Epoch: 2 [8/336 (2%)]\tMAE: 19.690710\tMSE: 408.154846\n","\n","Test set: Average MAE: 22.4662\t MSE: 604.4050\n","\n","Train Epoch: 3 [8/336 (2%)]\tMAE: 19.051525\tMSE: 377.168610\n","\n","Test set: Average MAE: 22.3361\t MSE: 598.6399\n","\n","Train Epoch: 4 [8/336 (2%)]\tMAE: 27.658222\tMSE: 886.686096\n","\n","Test set: Average MAE: 22.1821\t MSE: 591.7551\n","\n","Train Epoch: 5 [8/336 (2%)]\tMAE: 18.797462\tMSE: 367.118927\n","\n","Test set: Average MAE: 21.9980\t MSE: 583.4992\n","\n","Train Epoch: 6 [8/336 (2%)]\tMAE: 21.382277\tMSE: 498.104614\n","\n","Test set: Average MAE: 21.7820\t MSE: 573.8236\n","\n","Train Epoch: 7 [8/336 (2%)]\tMAE: 18.090143\tMSE: 386.925232\n","\n","Test set: Average MAE: 21.5244\t MSE: 562.3163\n","\n","Train Epoch: 8 [8/336 (2%)]\tMAE: 25.894602\tMSE: 812.688904\n","\n","Test set: Average MAE: 21.2249\t MSE: 548.9487\n","\n","Train Epoch: 9 [8/336 (2%)]\tMAE: 22.142128\tMSE: 556.778015\n","\n","Test set: Average MAE: 20.8801\t MSE: 533.8241\n","\n","Train Epoch: 10 [8/336 (2%)]\tMAE: 23.218956\tMSE: 577.750122\n","\n","Test set: Average MAE: 20.4853\t MSE: 516.5245\n","\n","Train Epoch: 11 [8/336 (2%)]\tMAE: 16.865200\tMSE: 322.135498\n","\n","Test set: Average MAE: 20.0241\t MSE: 496.4987\n","\n","Train Epoch: 12 [8/336 (2%)]\tMAE: 23.002218\tMSE: 643.990845\n","\n","Test set: Average MAE: 19.5113\t MSE: 474.5370\n","\n","Train Epoch: 13 [8/336 (2%)]\tMAE: 16.142038\tMSE: 349.307312\n","\n","Test set: Average MAE: 18.9341\t MSE: 450.3598\n","\n","Train Epoch: 14 [8/336 (2%)]\tMAE: 16.824711\tMSE: 337.289276\n","\n","Test set: Average MAE: 18.2985\t MSE: 424.2583\n","\n","Train Epoch: 15 [8/336 (2%)]\tMAE: 18.058426\tMSE: 441.208832\n","\n","Test set: Average MAE: 17.5960\t MSE: 396.3469\n","\n","Train Epoch: 16 [8/336 (2%)]\tMAE: 20.999338\tMSE: 493.291748\n","\n","Test set: Average MAE: 16.8184\t MSE: 366.4190\n","\n","Train Epoch: 17 [8/336 (2%)]\tMAE: 16.822311\tMSE: 371.993317\n","\n","Test set: Average MAE: 16.0063\t MSE: 336.7648\n","\n","Train Epoch: 18 [8/336 (2%)]\tMAE: 19.960306\tMSE: 531.002441\n","\n","Test set: Average MAE: 15.1411\t MSE: 305.8414\n","\n","Train Epoch: 19 [8/336 (2%)]\tMAE: 16.584955\tMSE: 327.860657\n","\n","Test set: Average MAE: 14.2478\t MSE: 275.1875\n","\n","Train Epoch: 20 [8/336 (2%)]\tMAE: 13.496176\tMSE: 302.916046\n","\n","Test set: Average MAE: 13.3190\t MSE: 245.4469\n","\n","Train Epoch: 21 [8/336 (2%)]\tMAE: 12.904400\tMSE: 264.076904\n","\n","Test set: Average MAE: 12.3983\t MSE: 217.8916\n","\n","Train Epoch: 22 [8/336 (2%)]\tMAE: 13.831303\tMSE: 237.545929\n","\n","Test set: Average MAE: 11.4902\t MSE: 192.6946\n","\n","Train Epoch: 23 [8/336 (2%)]\tMAE: 16.089624\tMSE: 381.462006\n","\n","Test set: Average MAE: 10.6411\t MSE: 170.1080\n","\n","Train Epoch: 24 [8/336 (2%)]\tMAE: 8.330772\tMSE: 77.188423\n","\n","Test set: Average MAE: 9.8639\t MSE: 150.0316\n","\n","Train Epoch: 25 [8/336 (2%)]\tMAE: 11.299048\tMSE: 135.487503\n","\n","Test set: Average MAE: 9.1983\t MSE: 133.3469\n","\n","Train Epoch: 26 [8/336 (2%)]\tMAE: 6.166327\tMSE: 54.353493\n","\n","Test set: Average MAE: 8.5925\t MSE: 118.9368\n","\n","Train Epoch: 27 [8/336 (2%)]\tMAE: 5.253368\tMSE: 38.993298\n","\n","Test set: Average MAE: 8.0854\t MSE: 107.1681\n","\n","Train Epoch: 28 [8/336 (2%)]\tMAE: 5.375408\tMSE: 37.601036\n","\n","Test set: Average MAE: 7.6502\t MSE: 97.7740\n","\n","Train Epoch: 29 [8/336 (2%)]\tMAE: 9.910948\tMSE: 201.313614\n","\n","Test set: Average MAE: 7.2249\t MSE: 89.5096\n","\n","Train Epoch: 30 [8/336 (2%)]\tMAE: 8.326818\tMSE: 168.634598\n","\n","Test set: Average MAE: 6.8624\t MSE: 82.9245\n","\n","Train Epoch: 31 [8/336 (2%)]\tMAE: 5.327443\tMSE: 34.682377\n","\n","Test set: Average MAE: 6.5808\t MSE: 77.6544\n","\n","Train Epoch: 32 [8/336 (2%)]\tMAE: 4.637916\tMSE: 28.226988\n","\n","Test set: Average MAE: 6.3396\t MSE: 73.0382\n","\n","Train Epoch: 33 [8/336 (2%)]\tMAE: 3.815263\tMSE: 21.518293\n","\n","Test set: Average MAE: 6.1611\t MSE: 69.5828\n","\n","Train Epoch: 34 [8/336 (2%)]\tMAE: 6.847089\tMSE: 55.408470\n","\n","Test set: Average MAE: 5.9951\t MSE: 66.1574\n","\n","Train Epoch: 35 [8/336 (2%)]\tMAE: 4.549287\tMSE: 42.003620\n","\n","Test set: Average MAE: 5.8459\t MSE: 63.3037\n","\n","Train Epoch: 36 [8/336 (2%)]\tMAE: 6.015549\tMSE: 45.655102\n","\n","Test set: Average MAE: 5.6922\t MSE: 60.5888\n","\n","Train Epoch: 37 [8/336 (2%)]\tMAE: 5.009172\tMSE: 49.201317\n","\n","Test set: Average MAE: 5.5706\t MSE: 58.6474\n","\n","Train Epoch: 38 [8/336 (2%)]\tMAE: 4.333414\tMSE: 66.736893\n","\n","Test set: Average MAE: 5.4583\t MSE: 56.9674\n","\n","Train Epoch: 39 [8/336 (2%)]\tMAE: 3.719714\tMSE: 19.563601\n","\n","Test set: Average MAE: 5.3227\t MSE: 55.0318\n","\n","Train Epoch: 40 [8/336 (2%)]\tMAE: 6.577093\tMSE: 105.555222\n","\n","Test set: Average MAE: 5.2054\t MSE: 53.3736\n","\n","Train Epoch: 41 [8/336 (2%)]\tMAE: 2.981787\tMSE: 13.725914\n","\n","Test set: Average MAE: 5.0978\t MSE: 51.9422\n","\n","Train Epoch: 42 [8/336 (2%)]\tMAE: 4.353915\tMSE: 29.953100\n","\n","Test set: Average MAE: 5.0292\t MSE: 51.0004\n","\n","Train Epoch: 43 [8/336 (2%)]\tMAE: 3.138698\tMSE: 14.602601\n","\n","Test set: Average MAE: 4.9527\t MSE: 49.7356\n","\n","Train Epoch: 44 [8/336 (2%)]\tMAE: 3.403669\tMSE: 20.549162\n","\n","Test set: Average MAE: 4.8999\t MSE: 48.9242\n","\n","Train Epoch: 45 [8/336 (2%)]\tMAE: 5.356281\tMSE: 43.720440\n","\n","Test set: Average MAE: 4.8281\t MSE: 47.7949\n","\n","Train Epoch: 46 [8/336 (2%)]\tMAE: 7.298956\tMSE: 82.343567\n","\n","Test set: Average MAE: 4.7798\t MSE: 47.0471\n","\n","Train Epoch: 47 [8/336 (2%)]\tMAE: 4.172723\tMSE: 23.675283\n","\n","Test set: Average MAE: 4.7427\t MSE: 46.4557\n","\n","Train Epoch: 48 [8/336 (2%)]\tMAE: 2.829151\tMSE: 11.215593\n","\n","Test set: Average MAE: 4.7017\t MSE: 45.7361\n","\n","Train Epoch: 49 [8/336 (2%)]\tMAE: 4.570217\tMSE: 28.441479\n","\n","Test set: Average MAE: 4.6694\t MSE: 45.2351\n","\n","Train Epoch: 50 [8/336 (2%)]\tMAE: 2.824866\tMSE: 11.145210\n","\n","Test set: Average MAE: 4.6410\t MSE: 44.7171\n","\n","Train Epoch: 51 [8/336 (2%)]\tMAE: 4.921987\tMSE: 43.401123\n","\n","Test set: Average MAE: 4.6138\t MSE: 44.1817\n","\n","Train Epoch: 52 [8/336 (2%)]\tMAE: 3.667774\tMSE: 16.882198\n","\n","Test set: Average MAE: 4.5923\t MSE: 43.7250\n","\n","Train Epoch: 53 [8/336 (2%)]\tMAE: 4.849573\tMSE: 35.810287\n","\n","Test set: Average MAE: 4.5696\t MSE: 43.2769\n","\n","Train Epoch: 54 [8/336 (2%)]\tMAE: 4.191886\tMSE: 27.720205\n","\n","Test set: Average MAE: 4.5420\t MSE: 42.9001\n","\n","Train Epoch: 55 [8/336 (2%)]\tMAE: 3.917635\tMSE: 19.070894\n","\n","Test set: Average MAE: 4.5275\t MSE: 42.4450\n","\n","Train Epoch: 56 [8/336 (2%)]\tMAE: 2.990717\tMSE: 15.782310\n","\n","Test set: Average MAE: 4.5072\t MSE: 42.1207\n","\n","Train Epoch: 57 [8/336 (2%)]\tMAE: 5.861578\tMSE: 97.597092\n","\n","Test set: Average MAE: 4.4953\t MSE: 41.7869\n","\n","Train Epoch: 58 [8/336 (2%)]\tMAE: 3.016350\tMSE: 13.303353\n","\n","Test set: Average MAE: 4.4771\t MSE: 41.4856\n","\n","Train Epoch: 59 [8/336 (2%)]\tMAE: 2.088892\tMSE: 10.408987\n","\n","Test set: Average MAE: 4.4585\t MSE: 41.1596\n","\n","Train Epoch: 60 [8/336 (2%)]\tMAE: 7.463062\tMSE: 143.354584\n","\n","Test set: Average MAE: 4.4441\t MSE: 40.9007\n","\n","Train Epoch: 61 [8/336 (2%)]\tMAE: 2.902079\tMSE: 10.780067\n","\n","Test set: Average MAE: 4.4287\t MSE: 40.5930\n","\n","Train Epoch: 62 [8/336 (2%)]\tMAE: 2.553751\tMSE: 9.258589\n","\n","Test set: Average MAE: 4.4146\t MSE: 40.3204\n","\n","Train Epoch: 63 [8/336 (2%)]\tMAE: 5.436682\tMSE: 71.789627\n","\n","Test set: Average MAE: 4.4042\t MSE: 40.0753\n","\n","Train Epoch: 64 [8/336 (2%)]\tMAE: 2.792615\tMSE: 10.071539\n","\n","Test set: Average MAE: 4.3899\t MSE: 39.8402\n","\n","Train Epoch: 65 [8/336 (2%)]\tMAE: 4.274165\tMSE: 33.422371\n","\n","Test set: Average MAE: 4.3796\t MSE: 39.6054\n","\n","Train Epoch: 66 [8/336 (2%)]\tMAE: 2.141582\tMSE: 7.321292\n","\n","Test set: Average MAE: 4.3654\t MSE: 39.3132\n","\n","Train Epoch: 67 [8/336 (2%)]\tMAE: 3.698964\tMSE: 27.206497\n","\n","Test set: Average MAE: 4.3531\t MSE: 39.0863\n","\n","Train Epoch: 68 [8/336 (2%)]\tMAE: 4.530254\tMSE: 43.003490\n","\n","Test set: Average MAE: 4.3365\t MSE: 38.8168\n","\n","Train Epoch: 69 [8/336 (2%)]\tMAE: 2.683435\tMSE: 11.885712\n","\n","Test set: Average MAE: 4.3237\t MSE: 38.6054\n","\n","Train Epoch: 70 [8/336 (2%)]\tMAE: 1.302492\tMSE: 2.909808\n","\n","Test set: Average MAE: 4.3106\t MSE: 38.3097\n","\n","Train Epoch: 71 [8/336 (2%)]\tMAE: 1.969656\tMSE: 5.965027\n","\n","Test set: Average MAE: 4.2957\t MSE: 38.2498\n","\n","Train Epoch: 72 [8/336 (2%)]\tMAE: 4.796525\tMSE: 78.484436\n","\n","Test set: Average MAE: 4.2856\t MSE: 37.8200\n","\n","Train Epoch: 73 [8/336 (2%)]\tMAE: 4.111816\tMSE: 24.133305\n","\n","Test set: Average MAE: 4.2737\t MSE: 37.7269\n","\n","Train Epoch: 74 [8/336 (2%)]\tMAE: 4.663196\tMSE: 32.284550\n","\n","Test set: Average MAE: 4.2645\t MSE: 37.4386\n","\n","Train Epoch: 75 [8/336 (2%)]\tMAE: 1.768276\tMSE: 5.260182\n","\n","Test set: Average MAE: 4.2515\t MSE: 37.3006\n","\n","Train Epoch: 76 [8/336 (2%)]\tMAE: 4.077188\tMSE: 70.781021\n","\n","Test set: Average MAE: 4.2415\t MSE: 36.9211\n","\n","Train Epoch: 77 [8/336 (2%)]\tMAE: 4.566513\tMSE: 47.624115\n","\n","Test set: Average MAE: 4.2284\t MSE: 36.7156\n","\n","Train Epoch: 78 [8/336 (2%)]\tMAE: 4.206416\tMSE: 26.646597\n","\n","Test set: Average MAE: 4.2165\t MSE: 36.6194\n","\n","Train Epoch: 79 [8/336 (2%)]\tMAE: 3.835499\tMSE: 26.189028\n","\n","Test set: Average MAE: 4.2020\t MSE: 36.3223\n","\n","Train Epoch: 80 [8/336 (2%)]\tMAE: 3.224639\tMSE: 15.474125\n","\n","Test set: Average MAE: 4.1866\t MSE: 36.1569\n","\n","Train Epoch: 81 [8/336 (2%)]\tMAE: 4.239422\tMSE: 37.674973\n","\n","Test set: Average MAE: 4.1767\t MSE: 35.9479\n","\n","Train Epoch: 82 [8/336 (2%)]\tMAE: 5.181961\tMSE: 32.207226\n","\n","Test set: Average MAE: 4.1642\t MSE: 35.6216\n","\n","Train Epoch: 83 [8/336 (2%)]\tMAE: 3.288911\tMSE: 16.703957\n","\n","Test set: Average MAE: 4.1530\t MSE: 35.4026\n","\n","Train Epoch: 84 [8/336 (2%)]\tMAE: 2.744920\tMSE: 10.849816\n","\n","Test set: Average MAE: 4.1378\t MSE: 35.2376\n","\n","Train Epoch: 85 [8/336 (2%)]\tMAE: 4.577480\tMSE: 34.106552\n","\n","Test set: Average MAE: 4.1182\t MSE: 35.0520\n","\n","Train Epoch: 86 [8/336 (2%)]\tMAE: 2.602406\tMSE: 12.633831\n","\n","Test set: Average MAE: 4.1093\t MSE: 34.8614\n","\n","Train Epoch: 87 [8/336 (2%)]\tMAE: 2.139036\tMSE: 8.030620\n","\n","Test set: Average MAE: 4.0931\t MSE: 34.6165\n","\n","Train Epoch: 88 [8/336 (2%)]\tMAE: 3.284516\tMSE: 22.790857\n","\n","Test set: Average MAE: 4.0818\t MSE: 34.3276\n","\n","Train Epoch: 89 [8/336 (2%)]\tMAE: 1.019892\tMSE: 1.486106\n","\n","Test set: Average MAE: 4.0681\t MSE: 34.1570\n","\n","Train Epoch: 90 [8/336 (2%)]\tMAE: 4.202894\tMSE: 29.855410\n","\n","Test set: Average MAE: 4.0521\t MSE: 33.9464\n","\n","Train Epoch: 91 [8/336 (2%)]\tMAE: 7.379833\tMSE: 121.827332\n","\n","Test set: Average MAE: 4.0393\t MSE: 33.7550\n","\n","Train Epoch: 92 [8/336 (2%)]\tMAE: 1.970852\tMSE: 4.852961\n","\n","Test set: Average MAE: 4.0255\t MSE: 33.5909\n","\n","Train Epoch: 93 [8/336 (2%)]\tMAE: 3.767956\tMSE: 20.219810\n","\n","Test set: Average MAE: 4.0164\t MSE: 33.4026\n","\n","Train Epoch: 94 [8/336 (2%)]\tMAE: 3.146910\tMSE: 19.676819\n","\n","Test set: Average MAE: 3.9985\t MSE: 33.1758\n","\n","Train Epoch: 95 [8/336 (2%)]\tMAE: 2.638721\tMSE: 11.635554\n","\n","Test set: Average MAE: 3.9871\t MSE: 32.9662\n","\n","Train Epoch: 96 [8/336 (2%)]\tMAE: 2.367394\tMSE: 10.717852\n","\n","Test set: Average MAE: 3.9751\t MSE: 32.7685\n","\n","Train Epoch: 97 [8/336 (2%)]\tMAE: 2.401140\tMSE: 7.270429\n","\n","Test set: Average MAE: 3.9603\t MSE: 32.5210\n","\n","Train Epoch: 98 [8/336 (2%)]\tMAE: 3.225919\tMSE: 14.107718\n","\n","Test set: Average MAE: 3.9484\t MSE: 32.4178\n","\n","Train Epoch: 99 [8/336 (2%)]\tMAE: 4.871456\tMSE: 42.463730\n","\n","Test set: Average MAE: 3.9323\t MSE: 32.0935\n","\n","Train Epoch: 100 [8/336 (2%)]\tMAE: 4.343987\tMSE: 32.518688\n","\n","Test set: Average MAE: 3.9188\t MSE: 31.9042\n","\n","Train Epoch: 101 [8/336 (2%)]\tMAE: 4.163048\tMSE: 33.367371\n","\n","Test set: Average MAE: 3.9031\t MSE: 31.7627\n","\n","Train Epoch: 102 [8/336 (2%)]\tMAE: 3.051223\tMSE: 13.792723\n","\n","Test set: Average MAE: 3.8896\t MSE: 31.4963\n","\n","Train Epoch: 103 [8/336 (2%)]\tMAE: 1.730894\tMSE: 4.276158\n","\n","Test set: Average MAE: 3.8753\t MSE: 31.2713\n","\n","Train Epoch: 104 [8/336 (2%)]\tMAE: 2.251589\tMSE: 7.142255\n","\n","Test set: Average MAE: 3.8603\t MSE: 31.0985\n","\n","Train Epoch: 105 [8/336 (2%)]\tMAE: 2.710589\tMSE: 8.982638\n","\n","Test set: Average MAE: 3.8469\t MSE: 30.9498\n","\n","Train Epoch: 106 [8/336 (2%)]\tMAE: 1.614582\tMSE: 7.587738\n","\n","Test set: Average MAE: 3.8316\t MSE: 30.7244\n","\n","Train Epoch: 107 [8/336 (2%)]\tMAE: 4.620970\tMSE: 31.260256\n","\n","Test set: Average MAE: 3.8176\t MSE: 30.5150\n","\n","Train Epoch: 108 [8/336 (2%)]\tMAE: 2.468585\tMSE: 8.886695\n","\n","Test set: Average MAE: 3.8031\t MSE: 30.3463\n","\n","Train Epoch: 109 [8/336 (2%)]\tMAE: 3.073494\tMSE: 17.092175\n","\n","Test set: Average MAE: 3.7859\t MSE: 30.1631\n","\n","Train Epoch: 110 [8/336 (2%)]\tMAE: 1.318722\tMSE: 2.328377\n","\n","Test set: Average MAE: 3.7718\t MSE: 29.8988\n","\n","Train Epoch: 111 [8/336 (2%)]\tMAE: 1.941221\tMSE: 6.069603\n","\n","Test set: Average MAE: 3.7564\t MSE: 29.7426\n","\n","Train Epoch: 112 [8/336 (2%)]\tMAE: 2.045099\tMSE: 6.674520\n","\n","Test set: Average MAE: 3.7449\t MSE: 29.5455\n","\n","Train Epoch: 113 [8/336 (2%)]\tMAE: 1.797065\tMSE: 5.004426\n","\n","Test set: Average MAE: 3.7247\t MSE: 29.3430\n","\n","Train Epoch: 114 [8/336 (2%)]\tMAE: 5.364523\tMSE: 96.871109\n","\n","Test set: Average MAE: 3.7078\t MSE: 29.1959\n","\n","Train Epoch: 115 [8/336 (2%)]\tMAE: 1.566841\tMSE: 3.173119\n","\n","Test set: Average MAE: 3.6936\t MSE: 28.9907\n","\n","Train Epoch: 116 [8/336 (2%)]\tMAE: 2.533882\tMSE: 10.767239\n","\n","Test set: Average MAE: 3.6779\t MSE: 28.8300\n","\n","Train Epoch: 117 [8/336 (2%)]\tMAE: 4.572678\tMSE: 51.298565\n","\n","Test set: Average MAE: 3.6619\t MSE: 28.6201\n","\n","Train Epoch: 118 [8/336 (2%)]\tMAE: 2.815890\tMSE: 15.959927\n","\n","Test set: Average MAE: 3.6460\t MSE: 28.4134\n","\n","Train Epoch: 119 [8/336 (2%)]\tMAE: 3.519618\tMSE: 21.426233\n","\n","Test set: Average MAE: 3.6313\t MSE: 28.2725\n","\n","Train Epoch: 120 [8/336 (2%)]\tMAE: 3.722351\tMSE: 24.098354\n","\n","Test set: Average MAE: 3.6145\t MSE: 28.0194\n","\n","Train Epoch: 121 [8/336 (2%)]\tMAE: 1.465075\tMSE: 3.376871\n","\n","Test set: Average MAE: 3.5997\t MSE: 27.8599\n","\n","Train Epoch: 122 [8/336 (2%)]\tMAE: 2.384922\tMSE: 9.310328\n","\n","Test set: Average MAE: 3.5810\t MSE: 27.6880\n","\n","Train Epoch: 123 [8/336 (2%)]\tMAE: 1.764866\tMSE: 5.339354\n","\n","Test set: Average MAE: 3.5655\t MSE: 27.5538\n","\n","Train Epoch: 124 [8/336 (2%)]\tMAE: 2.167294\tMSE: 9.627811\n","\n","Test set: Average MAE: 3.5538\t MSE: 27.3461\n","\n","Train Epoch: 125 [8/336 (2%)]\tMAE: 3.652306\tMSE: 26.650980\n","\n","Test set: Average MAE: 3.5391\t MSE: 27.2098\n","\n","Train Epoch: 126 [8/336 (2%)]\tMAE: 4.072432\tMSE: 22.896482\n","\n","Test set: Average MAE: 3.5271\t MSE: 27.0180\n","\n","Train Epoch: 127 [8/336 (2%)]\tMAE: 1.546303\tMSE: 4.822834\n","\n","Test set: Average MAE: 3.5109\t MSE: 26.8831\n","\n","Train Epoch: 128 [8/336 (2%)]\tMAE: 3.361792\tMSE: 13.543300\n","\n","Test set: Average MAE: 3.4951\t MSE: 26.6674\n","\n","Train Epoch: 129 [8/336 (2%)]\tMAE: 3.090609\tMSE: 15.089891\n","\n","Test set: Average MAE: 3.4826\t MSE: 26.5271\n","\n","Train Epoch: 130 [8/336 (2%)]\tMAE: 1.287794\tMSE: 2.953792\n","\n","Test set: Average MAE: 3.4699\t MSE: 26.3445\n","\n","Train Epoch: 131 [8/336 (2%)]\tMAE: 2.380674\tMSE: 6.577018\n","\n","Test set: Average MAE: 3.4516\t MSE: 26.1953\n","\n","Train Epoch: 132 [8/336 (2%)]\tMAE: 5.100351\tMSE: 79.325989\n","\n","Test set: Average MAE: 3.4365\t MSE: 26.0407\n","\n","Train Epoch: 133 [8/336 (2%)]\tMAE: 2.190566\tMSE: 5.999362\n","\n","Test set: Average MAE: 3.4213\t MSE: 25.8772\n","\n","Train Epoch: 134 [8/336 (2%)]\tMAE: 2.490777\tMSE: 11.263915\n","\n","Test set: Average MAE: 3.4086\t MSE: 25.7166\n","\n","Train Epoch: 135 [8/336 (2%)]\tMAE: 5.429900\tMSE: 80.128349\n","\n","Test set: Average MAE: 3.3971\t MSE: 25.5379\n","\n","Train Epoch: 136 [8/336 (2%)]\tMAE: 2.216716\tMSE: 6.941018\n","\n","Test set: Average MAE: 3.3770\t MSE: 25.4201\n","\n","Train Epoch: 137 [8/336 (2%)]\tMAE: 3.175146\tMSE: 20.390041\n","\n","Test set: Average MAE: 3.3644\t MSE: 25.2597\n","\n","Train Epoch: 138 [8/336 (2%)]\tMAE: 1.579485\tMSE: 3.531023\n","\n","Test set: Average MAE: 3.3503\t MSE: 25.0835\n","\n","Train Epoch: 139 [8/336 (2%)]\tMAE: 2.425435\tMSE: 10.031942\n","\n","Test set: Average MAE: 3.3421\t MSE: 24.9626\n","\n","Train Epoch: 140 [8/336 (2%)]\tMAE: 2.956922\tMSE: 10.430348\n","\n","Test set: Average MAE: 3.3204\t MSE: 24.8230\n","\n","Train Epoch: 141 [8/336 (2%)]\tMAE: 1.531620\tMSE: 2.819785\n","\n","Test set: Average MAE: 3.3070\t MSE: 24.6989\n","\n","Train Epoch: 142 [8/336 (2%)]\tMAE: 3.920779\tMSE: 22.432510\n","\n","Test set: Average MAE: 3.2917\t MSE: 24.5732\n","\n","Train Epoch: 143 [8/336 (2%)]\tMAE: 1.633445\tMSE: 4.102953\n","\n","Test set: Average MAE: 3.2810\t MSE: 24.4037\n","\n","Train Epoch: 144 [8/336 (2%)]\tMAE: 2.679827\tMSE: 11.468124\n","\n","Test set: Average MAE: 3.2708\t MSE: 24.2297\n","\n","Train Epoch: 145 [8/336 (2%)]\tMAE: 2.506140\tMSE: 9.629486\n","\n","Test set: Average MAE: 3.2551\t MSE: 24.1114\n","\n","Train Epoch: 146 [8/336 (2%)]\tMAE: 2.344824\tMSE: 7.394752\n","\n","Test set: Average MAE: 3.2425\t MSE: 23.9762\n","\n","Train Epoch: 147 [8/336 (2%)]\tMAE: 1.683111\tMSE: 4.254304\n","\n","Test set: Average MAE: 3.2318\t MSE: 23.8351\n","\n","Train Epoch: 148 [8/336 (2%)]\tMAE: 2.223968\tMSE: 10.216299\n","\n","Test set: Average MAE: 3.2133\t MSE: 23.7119\n","\n","Train Epoch: 149 [8/336 (2%)]\tMAE: 1.199811\tMSE: 2.406805\n","\n","Test set: Average MAE: 3.2004\t MSE: 23.5794\n","\n","Train Epoch: 150 [8/336 (2%)]\tMAE: 4.801143\tMSE: 68.757416\n","\n","Test set: Average MAE: 3.1849\t MSE: 23.4673\n","\n","\n","Test set: Average MAE: 3.5789\t MSE: 26.2428\n","\n"]}]},{"cell_type":"markdown","source":["----------\n","20250310\n","- 회귀 모델 제작 과정을 다 구현했다.\n","- hyperparameter\n","  - batch size는 data 수가 적어서 8, 16이 제일 잘 나온다\n","  - learning rate 1e-3, 1e-4가 제일 좋게 나오는 듯하다\n","\n","- model structure\n","  - 이제 내일 하자..\n","\n","- 총정리\n","  - 3/10 ~ 3/16 내로 하자"],"metadata":{"id":"6pWqqXhJOtAE"}},{"cell_type":"code","source":[],"metadata":{"id":"7Q5hTY5l7MY7"},"execution_count":null,"outputs":[]}]}